import os
import glob

import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence


# ========== æ¨¡å‹å®šä¹‰ï¼ˆæ¯”è®­ç»ƒä»£ç å¤šäº†ä¸€ä¸ªpredictï¼‰ ==========
class RnnClsNet(nn.Module):
    def __init__(
        self, vocab_size=5, embed_dim=5, hidden_dim=16, num_layers=2, cls_num=5
    ):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=-1)
        self.rnn = nn.RNN(
            embed_dim, hidden_dim, num_layers=num_layers, batch_first=True
        )
        self.fc = nn.Linear(hidden_dim, cls_num)

    def forward(self, x, seq_lengths):
        embedded = self.embedding(x)
        pack = pack_padded_sequence(
            embedded, seq_lengths, enforce_sorted=False, batch_first=True
        )
        out, h_n = self.rnn(pack)
        out = self.fc(h_n[-1])
        return out

    def predict(self, x):
        """
        è¾“å…¥ä¸€ä¸ªListåºåˆ—æˆ–è€…torch.Tensoråºåˆ—ï¼Œè¿”å›ç±»åˆ«å·
        x: Listæˆ–è€…torch.Tensor
        è¿”å›: cls_num
        """
        if isinstance(x, list):
            x = torch.tensor(x, dtype=torch.long).unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦
        elif isinstance(x, torch.Tensor):
            if x.dim() == 1:
                x = x.unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦
        else:
            raise TypeError("Input x must be a list or torch.Tensor")

        seq_lengths = torch.tensor([x.size(1)] * x.size(0), dtype=torch.long)

        with torch.no_grad():
            output = self.forward(x, seq_lengths)
            output = nn.functional.softmax(output, dim=1)
            pred = torch.argmax(output, dim=1)

        return pred.item() if pred.size(0) == 1 else pred


# ========== ä¸»ç¨‹åºï¼šåŠ è½½æ¨¡å‹ + ç»ˆç«¯äº¤äº’é¢„æµ‹ ==========
if __name__ == "__main__":
    # ===== é…ç½®è¶…å‚æ•° =====
    save_dir = "./output"

    # ===== æ¨¡å‹åˆå§‹åŒ– =====
    args = torch.load(os.path.join(save_dir, "args.pt"), weights_only=False)
    model = RnnClsNet(
        vocab_size=args.max_number
        + 1,  # è¾“å‡ºå’Œæ•°å­—å¯¹é½ï¼Œæ³¨æ„æ•°ç»„åŒ…å«0ï¼Œæœ‰max_number+1ä¸ªè¾“å…¥tokenï¼Œä¸‹åŒ
        embed_dim=args.embed_dim,
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers,
        cls_num=args.max_number + 1,
    )
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    # è¯»å–æœ€æ–°çš„chckpoints
    lastest_checkpoints = max(
        glob.glob(os.path.join(save_dir, "model_step_*.pt")),
        key=lambda x: int(x.split("_")[-1].split(".")[0]),
    )
    print(f"ä½¿ç”¨æ¨¡å‹æƒé‡ï¼š{lastest_checkpoints}")
    model.load_state_dict(
        torch.load(lastest_checkpoints, map_location=torch.device("cpu"))
    )  # åŠ è½½æƒé‡

    # ===== äº¤äº’å¼é¢„æµ‹ =====
    print(
        f"\nè¯·è¾“å…¥æ•°å­—åºåˆ—ï¼ˆç©ºæ ¼åˆ†éš”ï¼Œå¦‚ï¼š1 2 3ï¼‰ï¼Œæ³¨æ„è¾“å…¥æ•°å­—è¦åœ¨0-{args.max_number}ä¹‹é—´ï¼Œè¾“å…¥ 'quit' é€€å‡ºï¼š"
    )
    while True:
        user_input = input(">>> ").strip()
        if user_input.lower() == "quit":
            print("ğŸ‘‹ é€€å‡ºç¨‹åºã€‚")
            break

        try:
            # è§£æè¾“å…¥ä¸ºæ•´æ•°åˆ—è¡¨
            seq = list(map(int, user_input.split()))
            if len(seq) == 0:
                print("âš ï¸  è¾“å…¥ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
                continue
            # é¢„æµ‹
            pred_class = model.predict(seq)
            print(f"âŒ¨ï¸ è¾“å…¥åºåˆ—: {seq}")
            print(f"ğŸ¯ æ¨¡å‹é¢„æµ‹ç»“æœ: {pred_class}")
            print(
                f"âœ… è¾“å…¥åºåˆ—çš„æ±‚å’Œé™¤{args.max_number+1}ä½™æ•°ä¸º: {sum(seq)%(args.max_number+1)}"
            )

        except ValueError:
            print("âš ï¸  è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œè¯·è¾“å…¥ç©ºæ ¼åˆ†éš”çš„æ•´æ•°ï¼Œå¦‚ï¼š1 2 3")
        except Exception as e:
            print(f"âŒ é¢„æµ‹å‡ºé”™: {e}")
