import os
import argparse
import random
from functools import partial

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, IterableDataset
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence
import swanlab


# ========== 1. æ•°æ®éƒ¨åˆ† ==========


# å¯å˜é•¿æ•°æ®é›†ï¼ŒXä¸ºä¸€ä¸ª0-Nçš„åºåˆ—ï¼ŒYä¸ºè¯¥åºåˆ—çš„æ±‚å’Œé™¤N+1çš„ä½™æ•°ï¼ŒèŒƒå›´ä¹Ÿæ˜¯0-N
class SeqSumModDataset(IterableDataset):
    def __init__(self, total_samples=None, min_seq_len=2, max_seq_len=5, max_number=4):
        """
        :param total_samples: æ€»æ ·æœ¬æ•°ï¼ˆNone = æ— é™ç”Ÿæˆï¼‰
        :param min_len: æœ€çŸ­å­—æ¯åºåˆ—é•¿åº¦
        :param max_len: æœ€é•¿å­—æ¯åºåˆ—é•¿åº¦
        :param max_number: åºåˆ—ä¼šå‡ºç°çš„æœ€å¤§æ•°å­—
        """
        self.total_samples = total_samples
        self.min_len = min_seq_len
        self.max_len = max_seq_len
        self.max_number = max_number
        self.count = 0  # ç”¨äºé™åˆ¶æ ·æœ¬æ€»æ•°

    def __iter__(self):
        self.count = 0  # æ¯æ¬¡é‡æ–°è¿­ä»£æ—¶é‡ç½®è®¡æ•°å™¨
        return self

    def __next__(self):
        # ç”¨äºæ§åˆ¶epoch
        if self.total_samples is not None and self.count >= self.total_samples:
            raise StopIteration

        # åŠ¨æ€ç”Ÿæˆä¸€ä¸ªæ ·æœ¬
        seq_length = random.randint(self.min_len, self.max_len)
        input_seq = [random.randint(0, self.max_number) for _ in range(seq_length)]
        target_num = sum(input_seq) % (self.max_number + 1)
        self.count += 1
        return input_seq, seq_length, target_num


# ========== 2. æ¨¡å‹éƒ¨åˆ†ï¼ˆç¤ºä¾‹ï¼šå­—ç¬¦çº§ ç®€å•çš„RNNæ¨¡å‹ï¼‰ ==========


class RnnClsNet(nn.Module):
    def __init__(
        self, vocab_size=5, embed_dim=5, hidden_dim=16, num_layers=2, cls_num=5
    ):
        super().__init__()
        self.embedding = nn.Embedding(
            vocab_size + 1, embed_dim, padding_idx=-1
        )  # å¤šä¸€ä¸ªå¡«å……ä½
        self.rnn = nn.RNN(
            embed_dim, hidden_dim, num_layers=num_layers, batch_first=True
        )
        self.fc = nn.Linear(hidden_dim, cls_num)

    def forward(self, x, seq_lengths):
        # x: [batch_size, seq_len] ï¼ˆå­—ç¬¦ç´¢å¼•ï¼‰
        embedded = self.embedding(x)
        pack = pack_padded_sequence(
            embedded, seq_lengths, enforce_sorted=False, batch_first=True
        )
        out, h_n = self.rnn(pack)
        out = self.fc(h_n[-1])
        return out


# ========== 3. è®­ç»ƒè„šæœ¬ ==========


def train(args, device):
    # åˆå§‹åŒ–SwanLabè®°å½•æ—¥å¿—
    swanlab.init(experiment_name=args.run_name, config=args)

    # åˆ›å»ºæ•°æ®é›†
    dataset = SeqSumModDataset(  # è®­ç»ƒé›†ï¼Œæ— é™ä¸ª
        total_samples=None,
        min_seq_len=args.min_seq_len,
        max_seq_len=args.max_seq_len,
        max_number=args.max_number,
    )

    def pad_and_tensor(data, padding_value=5):
        """å¡«å……å‡½æ•°"""
        input_seqs, seq_lengths, target_nums = zip(*data)
        input_seqs = [torch.LongTensor(input_seq) for input_seq in input_seqs]
        seq_lengths = torch.LongTensor(seq_lengths)
        target_nums = torch.LongTensor(target_nums)
        input_seqs = pad_sequence(
            input_seqs, batch_first=True, padding_value=padding_value
        )
        return input_seqs, seq_lengths, target_nums

    pad_and_tensor = partial(pad_and_tensor, padding_value=args.max_number + 1)
    train_dataloader = DataLoader(
        dataset, batch_size=args.batch_size, collate_fn=pad_and_tensor
    )
    eval_dataset = SeqSumModDataset(  # æµ‹è¯•é›†ï¼Œä¸€å…±100ä¸ªï¼ˆæ³¨æ„æµ‹è¯•é›†æ ·æœ¬å¹¶éå›ºå®šï¼‰
        total_samples=100,
        min_seq_len=args.min_seq_len,
        max_seq_len=args.max_seq_len,
        max_number=args.max_number,
    )
    eval_dataloader = DataLoader(
        eval_dataset, batch_size=args.batch_size, collate_fn=pad_and_tensor
    )

    # åˆ›å»ºæ¨¡å‹
    model = RnnClsNet(
        vocab_size=args.max_number
        + 1,  # è¾“å‡ºå’Œæ•°å­—å¯¹é½ï¼Œæ³¨æ„æ•°ç»„åŒ…å«0ï¼Œæœ‰max_number+1ä¸ªè¾“å…¥tokenï¼Œä¸‹åŒ
        embed_dim=args.embed_dim,
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers,
        cls_num=args.max_number + 1,
    ).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    swanlab.log({"Model Params": total_params})
    model.train()

    # å‡†å¤‡ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss().to(device)  # ç¤ºä¾‹ç”¨å›å½’æŸå¤±
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # ä½™å¼¦å­¦ä¹ ç‡è¡°å‡
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=args.total_steps, eta_min=args.min_lr
    )

    if args.warmup_step > 0:
        # ç»„åˆè°ƒåº¦å™¨ï¼šå…ˆ warmupï¼Œå† cosine
        warmup_scheduler = optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=0.01,  # èµ·å§‹ç¼©æ”¾æ¯”ä¾‹
            end_factor=1.0,  # ç»“æŸæ—¶ä¸º 100%
            total_iters=args.warmup_step,
        )
        scheduler = optim.lr_scheduler.SequentialLR(
            optimizer,
            schedulers=[warmup_scheduler, scheduler],
            milestones=[args.warmup_step],  # åœ¨ç¬¬ warmup_epochs ä¸ª epoch ååˆ‡æ¢è°ƒåº¦å™¨
        )

    # å¼€å§‹è®­ç»ƒ
    os.makedirs(args.save_dir, exist_ok=True)
    torch.save(args, os.path.join(args.save_dir, "args.pt"))  # ä¿å­˜æ¨¡å‹å‚æ•°
    step = 0
    data_iter = iter(train_dataloader)
    while step < args.total_steps:
        try:
            batch = next(data_iter)
        except StopIteration:
            # å¦‚æœæ˜¯æœ‰é™æ•°æ®é›†ï¼Œé‡æ–°åˆ›å»ºè¿­ä»£å™¨
            data_iter = iter(train_dataloader)
            batch = next(data_iter)

        input_seqs, seq_lengths, target_nums = batch
        input_seqs = input_seqs.to(device)
        target_nums = target_nums.to(device)
        # ========== å‰å‘ + æŸå¤± + åå‘ ==========
        optimizer.zero_grad()
        outputs = model(input_seqs, seq_lengths)
        print
        loss = criterion(outputs, target_nums)
        loss.backward()
        optimizer.step()
        scheduler.step()
        step += 1

        # ========== è¯„ä¼° ==========
        if step % args.eval_every == 0:
            accuracy = eval_loop(model, eval_dataloader, device)
            print(f"##### Eval [{step}/{args.total_steps}], Acc: {accuracy:.4f}")
            swanlab.log({"accuracy": accuracy}, step=step)

        # ========== æ—¥å¿— ==========
        if step % args.log_every == 0:
            print(f"Step [{step}/{args.total_steps}], Loss: {loss.item():.4f}")
            current_lr = scheduler.get_last_lr()[0]
            swanlab.log({"loss": loss.item(), "current_lr": current_lr}, step=step)
        # ========== ä¿å­˜æ¨¡å‹ ==========
        if args.save_every and step % args.save_every == 0:
            torch.save(
                model.state_dict(), os.path.join(args.save_dir, f"model_step_{step}.pt")
            )
            path = os.path.join(args.save_dir, f"model_step_{step}.pt")
            print(f"æ¨¡å‹å·²ä¿å­˜: {path}")

    torch.save(model.state_dict(), os.path.join(args.save_dir, f"model_step_{step}.pt"))
    path = os.path.join(args.save_dir, f"model_step_{step}.pt")
    print(f"æ¨¡å‹å·²ä¿å­˜: {path}")
    print("è®­ç»ƒå®Œæˆï¼")


# ========== 4. è¯„ä¼°ç¨‹åº ==========


def eval_loop(model, dataloader, device):
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropoutã€BN ç­‰ï¼‰
    total_correct = 0
    total_samples = 0

    with torch.no_grad():  # æ•´ä¸ªå¾ªç¯æ”¾åœ¨ no_grad é‡Œæ›´é«˜æ•ˆ
        for batch in dataloader:
            input_seqs, seq_lengths, target_nums = batch

            input_seqs = input_seqs.to(device)
            target_nums = target_nums.to(device)

            # æ¨¡å‹æ¨ç†
            outputs = model(input_seqs, seq_lengths)
            # è·å–é¢„æµ‹ç±»åˆ«
            pred = torch.argmax(outputs, dim=1)  # shape: (batch_size,)
            # ç´¯è®¡æ­£ç¡®é¢„æµ‹æ•°å’Œæ ·æœ¬æ€»æ•°
            total_correct += (pred == target_nums).sum().item()
            total_samples += target_nums.size(0)

    model.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
    # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
    accuracy = total_correct / total_samples if total_samples > 0 else 0.0
    return accuracy


# ========== 5. å¯åŠ¨ç¨‹åº ==========

if __name__ == "__main__":
    # ä½¿ç”¨è®¾å¤‡
    device = "cpu"  # for debug
    # device = "mps"  # for macï¼ˆæˆ‘çš„M2ç”µè„‘å‘ç°ï¼Œç”¨è¿™ä¸ªæ¯”cpuæ¨¡å¼è¿˜æ…¢ğŸ˜­ï¼‰
    # device = "npu"  # for ascend

    # è¶…å‚æ•°
    parser = argparse.ArgumentParser(description="Training Configuration")
    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--min_seq_len", type=int, default=2)
    parser.add_argument("--max_seq_len", type=int, default=5)
    parser.add_argument("--max_number", type=int, default=9)
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--embed_dim", type=int, default=10)
    parser.add_argument("--hidden_dim", type=int, default=16)
    parser.add_argument("--num_layers", type=int, default=1)
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--total_steps", type=int, default=20000)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--lr", type=float, default=0.001)
    parser.add_argument("--min_lr", type=float, default=0.0001)
    parser.add_argument("--warmup_step", type=float, default=100)
    parser.add_argument("--log_every", type=int, default=100)
    parser.add_argument("--eval_every", type=int, default=500)
    parser.add_argument("--save_every", type=int, default=1000)
    parser.add_argument("--save_dir", type=str, default="./output/")
    parser.add_argument("--run_name", type=str, default="baseline-lr1e2")
    args = parser.parse_args()
    # å¼€å§‹è®­ç»ƒ
    train(args, device)
